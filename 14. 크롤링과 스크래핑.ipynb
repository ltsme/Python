{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52410e31",
   "metadata": {},
   "source": [
    "# 데이터 다운로드 하기\n",
    ": 인터넷 파일을 내 PC에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f6c7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 되었습니다.\n"
     ]
    }
   ],
   "source": [
    "#library\n",
    "import urllib.request\n",
    "\n",
    "#URL과 경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test1.png\"\n",
    "\n",
    "# #다운로드 하기 \n",
    "# urllib.request.urlretrieve(url, savename)\n",
    "# print(\"저장 되었습니다.\")\n",
    "\n",
    "# 다운로드 하기\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "# 파일로 저장하기\n",
    "with open (savename, \"wb\") as f:\n",
    "    f.write(mem)\n",
    "    print(\"저장 되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ede07",
   "metadata": {},
   "source": [
    "# BeatifulSoup로 Scraping하기\n",
    ": 간단하게 HTML과 XML에서 정보를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe230cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/yhj/opt/anaconda3/lib/python3.8/site-packages (4.9.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/yhj/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3002090e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html>\n",
      "<body>\n",
      "<h1>Header</h1>\n",
      "<p> Line 1을 서술 </p>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "<h1>Header</h1>\n",
      "<p> Line 1을 서술 </p>\n",
      "-------\n",
      "h1= Header\n",
      "h1= Header\n",
      "p1=  Line 1을 서술 \n",
      "p1=  Line 1을 서술 \n"
     ]
    }
   ],
   "source": [
    "# 기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <h1>Header</h1>\n",
    "                <p> Line 1을 서술 </p>\n",
    "            </body>\n",
    "        </html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# HTML 분석\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(soup)\n",
    "\n",
    "# 원하는 부분 추출\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "print(h1)\n",
    "print(p1)\n",
    "print(\"-------\")\n",
    "\n",
    "# TEXT만 출력\n",
    "print(\"h1=\", h1.string)\n",
    "print(\"h1=\", h1.text)\n",
    "print(\"p1=\", p1.string)\n",
    "print(\"p1=\", p1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dffde4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html>\n",
      "<body>\n",
      "<h1 id=\"title\">Header</h1>\n",
      "<p id=\"body\"> Line 1을 서술 </p>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "<h1 id=\"title\">Header</h1>\n"
     ]
    }
   ],
   "source": [
    "# 기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <h1 id =\"title\">Header</h1>\n",
    "                <p id = \"body\"> Line 1을 서술 </p>\n",
    "            </body>\n",
    "        </html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# HTML 분석\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(soup)\n",
    "\n",
    "# 원하는 부분 추출\n",
    "title = soup.find(id = 'title')\n",
    "body = soup.find(id = 'body')\n",
    "print(title)\n",
    "print(body)\n",
    "\n",
    "# TEXT만 출력\n",
    "print(\"title = \", title.string)\n",
    "print(\"body = \", body.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dd09b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None >>> http://www.naver.com\n",
      "None >>> http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "# 기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <li><a href=\"http://www.naver.com\"></a></li>\n",
    "                <li><a href=\"http://www.daum.net\"></a></li>\n",
    "            </body>\n",
    "        </html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# HTML 분석\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "# 원하는 부분 추출\n",
    "\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# 링크 목록 출력\n",
    "for link in links:\n",
    "#     print(link.string)\n",
    "#     print(link.attrs['href'])\n",
    "    href = link.attrs['href']\n",
    "    text = link.string\n",
    "    print(text, \">>>\", href)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e7d5262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "----------------------------------------------------------------------------------------------------\n",
      "○ (강수) 19일(월) 오후에는 비가 내리겠습니다. ○ (기온) 이번 예보기간 아침최저기온은 23~26도, 낮최고기온은 29~34도로 오늘(13일, 아침최저기온 22~26도, 낮최고기온 23~33도)과 비슷하거나 조금 높겠습니다. ○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다. ○ (주말전망) 17일(토)~18일(일)은 구름많겠고, 아침 기온은 23~24도, 낮 기온은 29~33도가 되겠습니다.  * 이번 예보기간 동안 내륙에는 낮최고기온이 33도 내외, 아침최저기온이 25도 내외로 오르는 곳이 많아 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다. * 또한, 북태평양고기압 위치에 따른 강수와 낮최고기온 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' (강수) 19일(월) 오후에는 비가 내리겠습니다. ',\n",
       " ' (기온) 이번 예보기간 아침최저기온은 23~26도, 낮최고기온은 29~34도로 오늘(13일, 아침최저기온 22~26도, 낮최고기온 23~33도)과 비슷하거나 조금 높겠습니다. ',\n",
       " ' (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다. ',\n",
       " ' (주말전망) 17일(토)~18일(일)은 구름많겠고, 아침 기온은 23~24도, 낮 기온은 29~33도가 되겠습니다.  * 이번 예보기간 동안 내륙에는 낮최고기온이 33도 내외, 아침최저기온이 25도 내외로 오르는 곳이 많아 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다. * 또한, 북태평양고기압 위치에 따른 강수와 낮최고기온 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기상청 자료 활용하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "\n",
    "#data 가져오기\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "# print(soup)\n",
    "\n",
    "# 원하는 데이터 추출\n",
    "title = soup.find('title').string\n",
    "print(title)\n",
    "wf = soup.find('wf').string\n",
    "# print(wf)\n",
    "\n",
    "# 데이터 정리하기\n",
    "list_wfs = list(wf)\n",
    "except_char = ['<','b','r','/','>']\n",
    "result = \"\"\n",
    "\n",
    "for lwf in list_wfs:\n",
    "    if lwf not in except_char:\n",
    "        result += lwf\n",
    "        \n",
    "print('-' * 100)\n",
    "print(result)\n",
    "type(result)\n",
    "result.split('○')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ec3f0",
   "metadata": {},
   "source": [
    "# CSS 선택자 사용\n",
    "soup.select_one() : css 선택자로 요소 하나를 추출.\n",
    "soup.select() : css 선택자로 요소 여러개를 리스트로 추출."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf62c328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위키 북스\n",
      "유니티 게임 이펙트 입문서\n",
      "스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "모던 웹 사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div id = \"meigen\">\n",
    "                    <h1>위키 북스</h1>\n",
    "                    <ul class = \"items\">\n",
    "                        <li>유니티 게임 이펙트 입문서</li>\n",
    "                        <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "                        <li>모던 웹 사이트 디자인의 정석</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 필요한 부분 css로 추출하기\n",
    "# 타이틀 부분 추출하기\n",
    "\n",
    "# id = #    class = .    > = 자손     \"  \" = 후손\n",
    "# div 입장에서 h1, ui는 자손이며, 그 아래의 li는 후손이다.\n",
    "\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string\n",
    "print(h1)\n",
    "\n",
    "#목록 부분 추출하기\n",
    "li_lists = soup.select(\"div#meigen > ul.items > li\")\n",
    "print(li_lists)\n",
    "\n",
    "for li in li_lists:\n",
    "    print(li.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f816a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 금융에서 환율 정보 추출하기\n",
    "# https://finance.naver.com/marketindex/\n",
    "# #exchangeList > li.on > a.head.usd > div > span.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b0d99db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd /krw :  1,143.90\n"
     ]
    }
   ],
   "source": [
    "# 미국 환율 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# print(soup)\n",
    "\n",
    "# 데이터 추출하기\n",
    "# price = soup.select_one(\"div.head_info > span.value\").string \n",
    "price = soup.select_one(\"span.value\").string\n",
    "print(\"usd /krw : \", price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10c3e549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 : 1,144.50\n",
      "일본 : 1,036.40\n",
      "유럽연합 : 1,357.72\n",
      "중국 : 176.88\n"
     ]
    }
   ],
   "source": [
    "# 미국 환율 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# print(soup)\n",
    "\n",
    "# 데이터 추출하기\n",
    "prices = soup.select(\"div.head_info > span.value\")\n",
    "# print(prices)\n",
    "\n",
    "# 미국, 일본, 유럽연합, 중국의 환율\n",
    "\n",
    "print(\"미국 :\", prices[0].string)\n",
    "print(\"일본 :\", prices[1].string)\n",
    "print(\"유럽연합 :\", prices[2].string)\n",
    "print(\"중국 :\", prices[3].string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6c4591c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  흰 그림자\n",
      "-  사랑스런 추억\n",
      "-  흐르는 거리\n",
      "-  봄\n",
      "-  참회록\n",
      "-  간(肝)\n",
      "-  위로\n",
      "-  팔복\n",
      "-  못자는밤\n",
      "-  달같이\n",
      "-  고추밭\n",
      "-  아우의 인상화\n",
      "-  사랑의 전당\n",
      "-  이적\n",
      "-  비오는 밤\n",
      "-  산골물\n",
      "-  유언\n",
      "-  창\n",
      "-  바다\n",
      "-  비로봉\n",
      "-  산협의 오후\n",
      "-  명상\n",
      "-  소낙비\n",
      "-  한난계\n",
      "-  풍경\n",
      "-  달밤\n",
      "-  장\n",
      "-  밤\n",
      "-  황혼이 바다가 되어\n",
      "-  아침\n",
      "-  빨래\n",
      "-  꿈은 깨어지고\n",
      "-  산림\n",
      "-  이런날\n",
      "-  산상\n",
      "-  양지쪽\n",
      "-  닭\n",
      "-  가슴 1\n",
      "-  가슴 2\n",
      "-  비둘기\n",
      "-  황혼\n",
      "-  남쪽 하늘\n",
      "-  창공\n",
      "-  거리에서\n",
      "-  삶과 죽음\n",
      "-  초한대\n",
      "-  산울림\n",
      "-  해바라기 얼굴\n",
      "-  귀뚜라미와 나와\n",
      "-  애기의 새벽\n",
      "-  햇빛·바람\n",
      "-  반디불\n",
      "-  둘 다\n",
      "-  거짓부리\n",
      "-  눈\n",
      "-  참새\n",
      "-  버선본\n",
      "-  편지\n",
      "-  봄\n",
      "-  무얼 먹구 사나\n",
      "-  굴뚝\n",
      "-  햇비\n",
      "-  빗자루\n",
      "-  기왓장 내외\n",
      "-  오줌싸개 지도\n",
      "-  병아리\n",
      "-  조개껍질\n",
      "-  겨울\n",
      "-  트루게네프의 언덕\n",
      "-  달을 쏘다\n",
      "-  별똥 떨어진 데\n",
      "-  화원에 꽃이 핀다\n",
      "-  종시\n"
     ]
    }
   ],
   "source": [
    "# Copy Selector : #mw-content-text > div.mw-parser-output > ul:nth-child(6) > li > ul > li:nth-child(1) > a\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "lylics_list = soup.select(\"#mw-content-text > div.mw-parser-output > ul:nth-child(6) > li > ul > li:nth-child(1) > a\")\n",
    "# 하나가 아닌 여러개 출력을 위해, a 부터 하나하나씩 올라가면서 다시 체크 한다.\n",
    "\n",
    "lylics_list = soup.select(\"#mw-content-text > div.mw-parser-output > ul > li > a\")\n",
    "# print(lylics)\n",
    "\n",
    "for a in lylics_list:\n",
    "    if a.string == \"증보판\":\n",
    "        continue\n",
    "    print(\"- \", a.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9101860",
   "metadata": {},
   "source": [
    "# 다음 영화 연간 순위 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "acf04581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"link_txt\" href=\"/moviedb/main?movieId=122091\">남산의 부장들</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=127897\">다만 악에서 구하소서</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=125372\">반도</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=131909\">히트맨</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=132861\">테넷</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=133855\">백두산</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=134684\">#살아있다</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=134698\">강철비2: 정상회담</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=129891\">담보</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=126072\">닥터 두리틀</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=135112\">삼진그룹 영어토익반</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=130428\">정직한 후보</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=131516\">도굴</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=122090\">클로젯</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=127894\">오케이 마담</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=125368\">해치지않아</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=122361\">천문: 하늘에 묻는다</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=126126\">결백</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=134091\">1917</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=127122\">작은 아씨들</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=132397\">미드웨이</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=128397\">시동</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=124456\">지푸라기라도 잡고 싶은 짐승들</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=121369\">미스터 주: 사라진 VIP</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=136221\">인비저블맨</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=129100\">나쁜 녀석들: 포에버</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=121616\">국제수사</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=128636\">침입자</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=115602\">스타워즈: 라이즈 오브 스카이워커</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=126030\">스파이 지니어스 </a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=112938\">이웃사촌</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=129134\">온워드: 단 하루의 기적</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=133189\">소리도 없이</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=127896\">버즈 오브 프레이(할리 퀸의 황홀한 해방)</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=115600\">원더 우먼 1984</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=93004\">겨울왕국 2</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=125231\">오! 문희</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=142669\">그린랜드</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=53080\">위대한 쇼맨</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=139357\">런</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=110139\">뮬란</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=132748\">내가 죽던 날</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=111292\">기생충</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=135716\">신비아파트 극장판 하늘도깨비 대 요르문간드</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=139533\">프리즌 이스케이프</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=110022\">검객</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=130427\">조제</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=125255\">사라진 시간</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=136058\">밤쉘: 세상을 바꾼 폭탄선언</a>, <a class=\"link_txt\" href=\"/moviedb/main?movieId=115601\">알라딘</a>]\n"
     ]
    }
   ],
   "source": [
    "#mainContent > div > div.box_boxoffice > ol > li:nth-child(1) > div > div.thumb_cont > strong > a\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://movie.daum.net/ranking/boxoffice/yearly\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "lylics_list = soup.select(\"#mainContent > div > div.box_boxoffice > ol > li > div > div.thumb_cont > strong > a\")\n",
    "\n",
    "print(lylics_list)\n",
    "\n",
    "for a in lylics_list:\n",
    "\n",
    "        continue\n",
    "    print(\"- \", a.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2fc708c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SelectorSyntaxError",
     "evalue": "Expected a selector at position 0\n  line 1:\n\n^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSelectorSyntaxError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-a81859ecd0c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 데이터 추출하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlylics_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/bs4/element.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, selector, namespaces, limit, **kwargs)\u001b[0m\n\u001b[1;32m   1867\u001b[0m             )\n\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1869\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoupsieve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1871\u001b[0m         \u001b[0;31m# We do this because it's more consistent and because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/soupsieve/__init__.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(select, tag, namespaces, limit, flags, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;34m\"\"\"Select the specified tags.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/soupsieve/__init__.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(pattern, namespaces, flags, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_css_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/soupsieve/css_parser.py\u001b[0m in \u001b[0;36m_cached_css_compile\u001b[0;34m(pattern, namespaces, custom, flags)\u001b[0m\n\u001b[1;32m    209\u001b[0m     return cm.SoupSieve(\n\u001b[1;32m    210\u001b[0m         \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mCSSParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_selectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_selectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mnamespaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mcustom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/soupsieve/css_parser.py\u001b[0m in \u001b[0;36mprocess_selectors\u001b[0;34m(self, index, flags)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0;34m\"\"\"Process selectors.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_selectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/soupsieve/css_parser.py\u001b[0m in \u001b[0;36mparse_selectors\u001b[0;34m(self, iselector, index, flags)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0;31m# We will always need to finish a selector when `:has()` is used as it leads with combining.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m             raise SelectorSyntaxError(\n\u001b[0m\u001b[1;32m    993\u001b[0m                 \u001b[0;34m'Expected a selector at position {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSelectorSyntaxError\u001b[0m: Expected a selector at position 0\n  line 1:\n\n^"
     ]
    }
   ],
   "source": [
    "# 다음 영화 박스오피스\n",
    "\n",
    "#mAside > div.aside_g.aside_ranking > ul > li.on > div > ol:nth-child(2) > li:nth-child(1) > strong > a\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://media.daum.net/digital\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 데이터 추출하기\n",
    "lylics_list = soup.select(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b96f290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9f4be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
